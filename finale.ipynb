{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finale.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN8JwY0CF-Eu"
      },
      "source": [
        "!git clone https://github.com/keras-team/keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGukiOkBGCu6"
      },
      "source": [
        "cd keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TTqi4-oGEkg"
      },
      "source": [
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVg2UNcOGJWQ"
      },
      "source": [
        "# Train a DNN model for prediction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import kerastuner as kt\n",
        "\n",
        "# upload the data base at the certain path\n",
        "data = pd.read_csv('/content/trainset.csv')  \n",
        "x_train = data.loc[0:506, ['A','B','C','D','E','F']]\n",
        "x_train=np.array(x_train)\n",
        "y_train = data.loc[0:506, ['target']]\n",
        "y_train=np.array(y_train)\n",
        "\n",
        "#Define model\n",
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "#Set the input layer\n",
        "  model.add(keras.layers.Flatten(input_shape=(6,1)))\n",
        "#Set dropout rate search space\n",
        "  drop_rate = hp.Choice('drop_rate', \n",
        "                            [0.0, 0.1, 0.2, 0.3, 0.4,])\n",
        "#Set activation function search space\n",
        "  activation = hp.Choice('activation', \n",
        "                            ['relu', 'tanh', 'sigmoid'])\n",
        "#In here, we tuner the number of layers using for loop\n",
        "  for i in range(hp.Int('num_layers', 2 , 5)):\n",
        "    model.add(keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
        "                      min_value=6,\n",
        "                      max_value=50,\n",
        "                      step=1),\n",
        "                activation= activation))\n",
        "  model.add(keras.layers.Dropout(rate=drop_rate))\n",
        "  model.add(keras.layers.Dense(1, activation='linear'))\n",
        "#  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
        "  model.compile(\n",
        "          optimizer= optimizer,\n",
        "          loss='mean_absolute_error',\n",
        "          metrics=['mean_absolute_error'])\n",
        "\n",
        "#          optimizer=tf.keras.optimizers.optimizer(lr=hp_learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "\n",
        "  return model\n",
        "tuner = kt.RandomSearch(model_builder,\n",
        "                     objective='val_mean_absolute_error',\n",
        "                     max_trials=5,\n",
        "                     executions_per_trial=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='3')\n",
        "# represent the search space\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# search the best hyperparameters\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(x_train,y_train,epochs=50,validation_split=0.2,callbacks=[stop_early])\n",
        "\n",
        "# recall the overall hyperparameters\n",
        "tuner.results_summary()\n",
        "\n",
        "# train model with the best hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "\n",
        "history=model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=1)\n",
        "\n",
        "\n",
        "# plot the figure illustrating the training loss and validation loss\n",
        "epochs = len(history.history['loss'])\n",
        "plt.plot(range(epochs), history.history['loss'], label='loss')\n",
        "plt.plot(range(epochs), history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "x = np.array([0.0063,2.3100,0.5380,6.5750,65.2000,4.0900])\n",
        "x=x.reshape(1,6)\n",
        "y=model(x)\n",
        "print(y)\n",
        "\n",
        "#manifest the model structure\n",
        "model.summary()\n",
        "\n",
        "#extract the best hyperparameters \n",
        "tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuX7O9EAIm_6"
      },
      "source": [
        "!pip3 install pygad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YukvvS4oIrLA"
      },
      "source": [
        "# Optimization for input features\n",
        "\n",
        "import numpy as np\n",
        "import pygad\n",
        "\n",
        "#initialization\n",
        "function_inputs = [1,1,1,1,1,1]\n",
        "\n",
        "#define fitness function\n",
        "def fitness_func(solution, solution_idx):\n",
        "    # solution[i] represeent the value of the (i+1)th parameter in solution\n",
        "    #solution=np.empty(shape=(1,6))\n",
        "    #solution= np.array().shape(6,1)\n",
        "    output=model(np.array(solution*function_inputs).reshape(1,6)) # model is built in keras.tuner\n",
        "    fitness =((1/output)).numpy()[0]\n",
        "    return fitness\n",
        "\n",
        "# Number of generations.\n",
        "num_generations = 50 \n",
        "# Number of solutions to be selected as parents in the mating pool.\n",
        "num_parents_mating = 5 \n",
        "# Number of solutions in the population.\n",
        "sol_per_pop = 10 \n",
        "  \n",
        "# Type of parent selection.\n",
        "parent_selection_type = \"sss\"\n",
        "# Type of the crossover operator.\n",
        "crossover_type = \"single_point\" \n",
        "# Type of the mutation operator.\n",
        "mutation_type = \"random\"\n",
        "\n",
        "last_fitness = 0\n",
        "def callback_generation(ga_instance):\n",
        "    global last_fitness\n",
        "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
        "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
        "    print(\"Change     = {change}\".format(change=ga_instance.best_solution()[1] - last_fitness))\n",
        "    last_fitness = ga_instance.best_solution()[1]\n",
        "\n",
        "# Creating an instance of the GA class inside the ga module. Some parameters are initialized within the constructor.\n",
        "ga_instance = pygad.GA(\n",
        "     #Number of generations(iterations).\n",
        "     num_generations=num_generations,\n",
        "     num_parents_mating=num_parents_mating,\n",
        "     fitness_func=fitness_func,\n",
        "     sol_per_pop=sol_per_pop,\n",
        "     #the Number of parameters(the number of genes)\n",
        "     num_genes=6,\n",
        "     gene_space=[np.arange(0,10,0.01).tolist(),np.arange(0,15,0.01).tolist(),np.arange(0,21,0.01).tolist(),np.arange(0,2,0.01).tolist(),\n",
        "                 np.arange(0,100,0.01).tolist(),np.arange(0,2,0.01).tolist()],\n",
        "     \n",
        "     parent_selection_type=parent_selection_type,\n",
        "     keep_parents=1,\n",
        "     crossover_type=crossover_type,\n",
        "     mutation_percent_genes=50,\n",
        "     mutation_type=mutation_type,\n",
        "     callback_generation=callback_generation)\n",
        "\n",
        "# Running the GA to optimize the parameters of the function.\n",
        "ga_instance.run()\n",
        "\n",
        "\n",
        "# After the generations complete, some plots are showed that summarize the how the outputs/fitenss values evolve over generations.\n",
        "ga_instance.plot_result()\n",
        "\n",
        "# Returning the details of the best solution.\n",
        "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
        "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=1/solution_fitness))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
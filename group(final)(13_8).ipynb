{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "group(final)(13.8).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkGdn965v5x0FHcepWEzLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zengxuhe/pyGAD-kerastuner/blob/main/group(final)(13_8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15enpKdZlo8Y",
        "outputId": "6e8eb381-66f3-4519-fba4-3131bddbe916"
      },
      "source": [
        "!git clone https://github.com/keras-team/keras-tuner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-tuner'...\n",
            "remote: Enumerating objects: 7099, done.\u001b[K\n",
            "remote: Counting objects: 100% (457/457), done.\u001b[K\n",
            "remote: Compressing objects: 100% (205/205), done.\u001b[K\n",
            "remote: Total 7099 (delta 252), reused 397 (delta 231), pack-reused 6642\u001b[K\n",
            "Receiving objects: 100% (7099/7099), 1.53 MiB | 4.34 MiB/s, done.\n",
            "Resolving deltas: 100% (4957/4957), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhP4T0X6lqw8",
        "outputId": "a6cbde1c-b527-4fbe-bdd8-519b0c1a99be"
      },
      "source": [
        "cd keras-tuner"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-tuner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF9WBjQXlqzc",
        "outputId": "39fff011-f308-47e8-f1ba-9a527af91760"
      },
      "source": [
        "!pip install ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/keras-tuner\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner==1.0.3) (21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner==1.0.3) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner==1.0.3) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner==1.0.3) (1.4.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner==1.0.3) (2.5.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner==1.0.3) (5.5.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt-legacy-1.0.3.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner==1.0.3) (57.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner==1.0.3) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner==1.0.3) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner==1.0.3) (5.0.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner==1.0.3) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner==1.0.3) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner==1.0.3) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner==1.0.3) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras-tuner==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner==1.0.3) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner==1.0.3) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner==1.0.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner==1.0.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner==1.0.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner==1.0.3) (2021.5.30)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.34.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.32.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner==1.0.3) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner==1.0.3) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner==1.0.3) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner==1.0.3) (3.7.4.3)\n",
            "Building wheels for collected packages: keras-tuner, kt-legacy\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.3-py3-none-any.whl size=97186 sha256=911ba96e87398d399d8a3316b4ca1bab13d164cff6dd23cd67cbebd493b75257\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/f4/56/f120140a3c0706aebedf4471bfee8f02bbce4755424e32e245\n",
            "  Building wheel for kt-legacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kt-legacy: filename=kt_legacy-1.0.3-py3-none-any.whl size=9568 sha256=35fa2a5175cb9af3cb5e6999c3c2f2e8c56564008fd5cca69645d955056d10e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/5c/e0/13003e68c17f403af40b92a24d20171b95fef13b0fdaba833c\n",
            "Successfully built keras-tuner kt-legacy\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.0.3 kt-legacy-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxv5vPvMlq2D"
      },
      "source": [
        "# Train a DNN model for prediction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import kerastuner as kt\n",
        "\n",
        "# upload the data base at the certain path\n",
        "data = pd.read_csv('/content/traincmp.csv')\n",
        "\n",
        "\n",
        "x = data.loc[0:119, ['A','B','C','D','E']]\n",
        "x =np.array(x)\n",
        "x_mean = np.mean(x,axis=0)\n",
        "x_std = np.std(x,axis=0)\n",
        "x = (x-x_mean)/x_std\n",
        "x_test = x[110:119,:]\n",
        "x = x [0:110,:]\n",
        "\n",
        "\n",
        "y = data.loc[0:119, ['target']]\n",
        "y=np.array(y)\n",
        "y_target= y[110:119,:]\n",
        "y_mean = np.mean(y,axis=0)\n",
        "y_std = np.std(y,axis=0)\n",
        "y = (y-y_mean)/y_std\n",
        "y = y [0:110,:]\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(3)\n",
        "np.random.shuffle(x)\n",
        "np.random.seed(3)\n",
        "np.random.shuffle(y)\n",
        "\n",
        "x_train = x[0:100,:]\n",
        "x_val = x[100:110,:]\n",
        "y_train = y[0:100,:]\n",
        "y_val = y[100:110,:]\n",
        "\n",
        "\n",
        "#Define model\n",
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "#Set the input layer\n",
        "  model.add(keras.layers.Flatten(input_shape=(5,1)))\n",
        "#Set dropout rate search space\n",
        "  drop_rate = hp.Choice('drop_rate', \n",
        "                            [0.0, 0.1, 0.2, 0.3, 0.4,])\n",
        "#Set activation function search space\n",
        "  activation = hp.Choice('activation', \n",
        "                            ['relu', 'tanh', 'sigmoid'])\n",
        "#In here, we tuner the number of layers using for loop\n",
        "  for i in range(hp.Int('num_layers', 2 , 5)):\n",
        "    model.add(keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
        "                      min_value=5,\n",
        "                      max_value=50,\n",
        "                      step=1),\n",
        "                activation= activation))\n",
        "  model.add(keras.layers.Dropout(rate=drop_rate))\n",
        "  model.add(keras.layers.Dense(1, activation='linear'))\n",
        "#  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
        "  model.compile(\n",
        "          optimizer= optimizer,\n",
        "          loss='mean_absolute_error',\n",
        "          metrics=['mean_absolute_error'])\n",
        "\n",
        "#          optimizer=tf.keras.optimizers.optimizer(lr=hp_learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "\n",
        "  return model\n",
        "tuner = kt.RandomSearch(model_builder,\n",
        "                     objective='val_mean_absolute_error',\n",
        "                     max_trials=5,\n",
        "                     executions_per_trial=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='255245112154156315312654245298')\n",
        "# represent the search space\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# search the best hyperparameters\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "tuner.search(x_train,y_train,epochs=50,validation_split=0.2,callbacks=[stop_early])\n",
        "\n",
        "# recall the overall hyperparameters\n",
        "tuner.results_summary()\n",
        "\n",
        "# train model with the best hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "\n",
        "history=model.fit(x_train, y_train, batch_size=16, epochs=300, validation_data=(x_val,y_val), validation_freq=1, shuffle=False)\n",
        "\n",
        "\n",
        "# plot the figure illustrating the training loss and validation loss\n",
        "epochs = len(history.history['loss'])\n",
        "plt.plot(range(epochs), history.history['loss'], label='loss')\n",
        "plt.plot(range(epochs), history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "x = np.array([10,10,65,50,5])\n",
        "x=x.reshape(1,5)\n",
        "y=model(x)\n",
        "print(y)\n",
        "#manifest the model structure\n",
        "model.summary()\n",
        "\n",
        "#extract the best hyperparameters \n",
        "tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KC2LpJ7Ylq4j",
        "outputId": "868e570b-ed10-4078-d750-ab45a98106d2"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import kerastuner as kt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# upload the data base at the certain path\n",
        "data = pd.read_csv('/content/traincmp.csv')\n",
        "\n",
        "\n",
        "x = data.loc[0:119, ['A','B','C','D','E']]\n",
        "x =np.array(x)\n",
        "x_mean = np.mean(x,axis=0)\n",
        "x_std = np.std(x,axis=0)\n",
        "x = (x-x_mean)/x_std\n",
        "x_test = x[110:119,:]\n",
        "x = x [0:110,:]\n",
        "\n",
        "\n",
        "y = data.loc[0:119, ['target']]\n",
        "y=np.array(y)\n",
        "y_target= y[110:119,:]\n",
        "y_mean = np.mean(y,axis=0)\n",
        "y_std = np.std(y,axis=0)\n",
        "y = (y-y_mean)/y_std\n",
        "y = y [0:110,:]\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(3)\n",
        "np.random.shuffle(x)\n",
        "np.random.seed(3)\n",
        "np.random.shuffle(y)\n",
        "\n",
        "x_train = x[0:100,:]\n",
        "x_val = x[100:110,:]\n",
        "y_train = y[0:100,:]\n",
        "y_val = y[100:110,:]\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(5,1)),\n",
        "        tf.keras.layers.Dense(46, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(34, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(28, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(33, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "#SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "#RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
        "#Adagrad(lr=0.01, epsilon=1e-06)\n",
        "#Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "          loss='mae',\n",
        "          metrics=['mae'])\n",
        "\n",
        "history=model.fit(x_train, y_train, batch_size=16, epochs=300, validation_data=(x_val,y_val), validation_freq=1, shuffle=False)\n",
        "#history=model.fit(x_train, y_train, batch_size=16, epochs=200, validation_split=0.2, validation_freq=1,shuffle=False)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "epochs = len(history.history['loss'])\n",
        "plt.plot(np.arange(len(history.history['loss'])),history.history['loss'],label='training')\n",
        "plt.plot(np.arange(len(history.history['val_loss'])),history.history['val_loss'],label='validation')\n",
        "plt.legend(fontsize=10)\n",
        "plt.title('learning curve',fontsize=15)\n",
        "plt.xlabel('iteration',fontsize=15)\n",
        "plt.ylabel('error',fontsize=15)\n",
        "plt.show()\n",
        "y_test=model(x_test)\n",
        "y_mid= tf.multiply(y_test,y_std)\n",
        "y_test= tf.add(y_mid, y_mean)\n",
        "error= tf.abs(tf.subtract(y_test, y_target))\n",
        "error_percentage = tf.divide(error, y_target)\n",
        "print(error)\n",
        "print(error_percentage)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.6610 - mae: 0.6610 - val_loss: 0.9406 - val_mae: 0.9406\n",
            "Epoch 2/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.6159 - mae: 0.6159 - val_loss: 0.8943 - val_mae: 0.8943\n",
            "Epoch 3/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.5883 - mae: 0.5883 - val_loss: 0.8694 - val_mae: 0.8694\n",
            "Epoch 4/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5622 - mae: 0.5622 - val_loss: 0.8280 - val_mae: 0.8280\n",
            "Epoch 5/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.5522 - mae: 0.5522 - val_loss: 0.7959 - val_mae: 0.7959\n",
            "Epoch 6/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.5470 - mae: 0.5470 - val_loss: 0.7776 - val_mae: 0.7776\n",
            "Epoch 7/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5265 - mae: 0.5265 - val_loss: 0.7630 - val_mae: 0.7630\n",
            "Epoch 8/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.5152 - mae: 0.5152 - val_loss: 0.7452 - val_mae: 0.7452\n",
            "Epoch 9/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.4948 - mae: 0.4948 - val_loss: 0.7283 - val_mae: 0.7283\n",
            "Epoch 10/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.4789 - mae: 0.4789 - val_loss: 0.7169 - val_mae: 0.7169\n",
            "Epoch 11/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4594 - mae: 0.4594 - val_loss: 0.6990 - val_mae: 0.6990\n",
            "Epoch 12/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4528 - mae: 0.4528 - val_loss: 0.6811 - val_mae: 0.6811\n",
            "Epoch 13/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4236 - mae: 0.4236 - val_loss: 0.6405 - val_mae: 0.6405\n",
            "Epoch 14/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4139 - mae: 0.4139 - val_loss: 0.6196 - val_mae: 0.6196\n",
            "Epoch 15/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4002 - mae: 0.4002 - val_loss: 0.6057 - val_mae: 0.6057\n",
            "Epoch 16/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.3836 - mae: 0.3836 - val_loss: 0.6002 - val_mae: 0.6002\n",
            "Epoch 17/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.3535 - mae: 0.3535 - val_loss: 0.5701 - val_mae: 0.5701\n",
            "Epoch 18/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.3508 - mae: 0.3508 - val_loss: 0.5295 - val_mae: 0.5295\n",
            "Epoch 19/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3494 - mae: 0.3494 - val_loss: 0.5001 - val_mae: 0.5001\n",
            "Epoch 20/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3183 - mae: 0.3183 - val_loss: 0.4838 - val_mae: 0.4838\n",
            "Epoch 21/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3088 - mae: 0.3088 - val_loss: 0.4743 - val_mae: 0.4743\n",
            "Epoch 22/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2866 - mae: 0.2866 - val_loss: 0.4700 - val_mae: 0.4700\n",
            "Epoch 23/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2819 - mae: 0.2819 - val_loss: 0.4694 - val_mae: 0.4694\n",
            "Epoch 24/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2603 - mae: 0.2603 - val_loss: 0.4413 - val_mae: 0.4413\n",
            "Epoch 25/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2691 - mae: 0.2691 - val_loss: 0.4376 - val_mae: 0.4376\n",
            "Epoch 26/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.2546 - mae: 0.2546 - val_loss: 0.4198 - val_mae: 0.4198\n",
            "Epoch 27/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.2188 - mae: 0.2188 - val_loss: 0.4080 - val_mae: 0.4080\n",
            "Epoch 28/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.2192 - mae: 0.2192 - val_loss: 0.4339 - val_mae: 0.4339\n",
            "Epoch 29/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2120 - mae: 0.2120 - val_loss: 0.4079 - val_mae: 0.4079\n",
            "Epoch 30/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.2158 - mae: 0.2158 - val_loss: 0.3899 - val_mae: 0.3899\n",
            "Epoch 31/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.2055 - mae: 0.2055 - val_loss: 0.4100 - val_mae: 0.4100\n",
            "Epoch 32/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1996 - mae: 0.1996 - val_loss: 0.3627 - val_mae: 0.3627\n",
            "Epoch 33/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1804 - mae: 0.1804 - val_loss: 0.3650 - val_mae: 0.3650\n",
            "Epoch 34/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1932 - mae: 0.1932 - val_loss: 0.4085 - val_mae: 0.4085\n",
            "Epoch 35/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1950 - mae: 0.1950 - val_loss: 0.3965 - val_mae: 0.3965\n",
            "Epoch 36/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1689 - mae: 0.1689 - val_loss: 0.3499 - val_mae: 0.3499\n",
            "Epoch 37/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1869 - mae: 0.1869 - val_loss: 0.3626 - val_mae: 0.3626\n",
            "Epoch 38/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1627 - mae: 0.1627 - val_loss: 0.3730 - val_mae: 0.3730\n",
            "Epoch 39/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1755 - mae: 0.1755 - val_loss: 0.3534 - val_mae: 0.3534\n",
            "Epoch 40/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1790 - mae: 0.1790 - val_loss: 0.3694 - val_mae: 0.3694\n",
            "Epoch 41/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1774 - mae: 0.1774 - val_loss: 0.3379 - val_mae: 0.3379\n",
            "Epoch 42/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1709 - mae: 0.1709 - val_loss: 0.3033 - val_mae: 0.3033\n",
            "Epoch 43/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1673 - mae: 0.1673 - val_loss: 0.3381 - val_mae: 0.3381\n",
            "Epoch 44/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1804 - mae: 0.1804 - val_loss: 0.3331 - val_mae: 0.3331\n",
            "Epoch 45/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1678 - mae: 0.1678 - val_loss: 0.3078 - val_mae: 0.3078\n",
            "Epoch 46/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1475 - mae: 0.1475 - val_loss: 0.3108 - val_mae: 0.3108\n",
            "Epoch 47/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1625 - mae: 0.1625 - val_loss: 0.3493 - val_mae: 0.3493\n",
            "Epoch 48/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1668 - mae: 0.1668 - val_loss: 0.3019 - val_mae: 0.3019\n",
            "Epoch 49/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1736 - mae: 0.1736 - val_loss: 0.3183 - val_mae: 0.3183\n",
            "Epoch 50/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1574 - mae: 0.1574 - val_loss: 0.3471 - val_mae: 0.3471\n",
            "Epoch 51/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1487 - mae: 0.1487 - val_loss: 0.2999 - val_mae: 0.2999\n",
            "Epoch 52/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1634 - mae: 0.1634 - val_loss: 0.3128 - val_mae: 0.3128\n",
            "Epoch 53/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1619 - mae: 0.1619 - val_loss: 0.3012 - val_mae: 0.3012\n",
            "Epoch 54/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1512 - mae: 0.1512 - val_loss: 0.3232 - val_mae: 0.3232\n",
            "Epoch 55/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1736 - mae: 0.1736 - val_loss: 0.3077 - val_mae: 0.3077\n",
            "Epoch 56/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1480 - mae: 0.1480 - val_loss: 0.2893 - val_mae: 0.2893\n",
            "Epoch 57/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1608 - mae: 0.1608 - val_loss: 0.3262 - val_mae: 0.3262\n",
            "Epoch 58/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1473 - mae: 0.1473 - val_loss: 0.2664 - val_mae: 0.2664\n",
            "Epoch 59/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1694 - mae: 0.1694 - val_loss: 0.2953 - val_mae: 0.2953\n",
            "Epoch 60/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1541 - mae: 0.1541 - val_loss: 0.2635 - val_mae: 0.2635\n",
            "Epoch 61/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1602 - mae: 0.1602 - val_loss: 0.2688 - val_mae: 0.2688\n",
            "Epoch 62/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1477 - mae: 0.1477 - val_loss: 0.2744 - val_mae: 0.2744\n",
            "Epoch 63/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1410 - mae: 0.1410 - val_loss: 0.2762 - val_mae: 0.2762\n",
            "Epoch 64/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1365 - mae: 0.1365 - val_loss: 0.2910 - val_mae: 0.2910\n",
            "Epoch 65/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1376 - mae: 0.1376 - val_loss: 0.2721 - val_mae: 0.2721\n",
            "Epoch 66/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1365 - mae: 0.1365 - val_loss: 0.2730 - val_mae: 0.2730\n",
            "Epoch 67/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1580 - mae: 0.1580 - val_loss: 0.2745 - val_mae: 0.2745\n",
            "Epoch 68/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1525 - mae: 0.1525 - val_loss: 0.2852 - val_mae: 0.2852\n",
            "Epoch 69/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1282 - mae: 0.1282 - val_loss: 0.2583 - val_mae: 0.2583\n",
            "Epoch 70/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1364 - mae: 0.1364 - val_loss: 0.2708 - val_mae: 0.2708\n",
            "Epoch 71/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1146 - mae: 0.1146 - val_loss: 0.2493 - val_mae: 0.2493\n",
            "Epoch 72/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1452 - mae: 0.1452 - val_loss: 0.2593 - val_mae: 0.2593\n",
            "Epoch 73/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1252 - mae: 0.1252 - val_loss: 0.2531 - val_mae: 0.2531\n",
            "Epoch 74/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1592 - mae: 0.1592 - val_loss: 0.2743 - val_mae: 0.2743\n",
            "Epoch 75/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1441 - mae: 0.1441 - val_loss: 0.2476 - val_mae: 0.2476\n",
            "Epoch 76/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1241 - mae: 0.1241 - val_loss: 0.2438 - val_mae: 0.2438\n",
            "Epoch 77/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1324 - mae: 0.1324 - val_loss: 0.2480 - val_mae: 0.2480\n",
            "Epoch 78/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1213 - mae: 0.1213 - val_loss: 0.2484 - val_mae: 0.2484\n",
            "Epoch 79/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1249 - mae: 0.1249 - val_loss: 0.2449 - val_mae: 0.2449\n",
            "Epoch 80/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1176 - mae: 0.1176 - val_loss: 0.2501 - val_mae: 0.2501\n",
            "Epoch 81/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1317 - mae: 0.1317 - val_loss: 0.2680 - val_mae: 0.2680\n",
            "Epoch 82/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1487 - mae: 0.1487 - val_loss: 0.2431 - val_mae: 0.2431\n",
            "Epoch 83/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1295 - mae: 0.1295 - val_loss: 0.2765 - val_mae: 0.2765\n",
            "Epoch 84/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1220 - mae: 0.1220 - val_loss: 0.2275 - val_mae: 0.2275\n",
            "Epoch 85/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1472 - mae: 0.1472 - val_loss: 0.2135 - val_mae: 0.2135\n",
            "Epoch 86/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1362 - mae: 0.1362 - val_loss: 0.2181 - val_mae: 0.2181\n",
            "Epoch 87/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1122 - mae: 0.1122 - val_loss: 0.2139 - val_mae: 0.2139\n",
            "Epoch 88/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1202 - mae: 0.1202 - val_loss: 0.2094 - val_mae: 0.2094\n",
            "Epoch 89/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1439 - mae: 0.1439 - val_loss: 0.2586 - val_mae: 0.2586\n",
            "Epoch 90/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1219 - mae: 0.1219 - val_loss: 0.2538 - val_mae: 0.2538\n",
            "Epoch 91/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1236 - mae: 0.1236 - val_loss: 0.2401 - val_mae: 0.2401\n",
            "Epoch 92/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1306 - mae: 0.1306 - val_loss: 0.2403 - val_mae: 0.2403\n",
            "Epoch 93/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1273 - mae: 0.1273 - val_loss: 0.2196 - val_mae: 0.2196\n",
            "Epoch 94/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1153 - mae: 0.1153 - val_loss: 0.2247 - val_mae: 0.2247\n",
            "Epoch 95/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0953 - mae: 0.0953 - val_loss: 0.2020 - val_mae: 0.2020\n",
            "Epoch 96/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1061 - mae: 0.1061 - val_loss: 0.2196 - val_mae: 0.2196\n",
            "Epoch 97/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1110 - mae: 0.1110 - val_loss: 0.2195 - val_mae: 0.2195\n",
            "Epoch 98/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1190 - mae: 0.1190 - val_loss: 0.2303 - val_mae: 0.2303\n",
            "Epoch 99/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1035 - mae: 0.1035 - val_loss: 0.2157 - val_mae: 0.2157\n",
            "Epoch 100/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1008 - mae: 0.1008 - val_loss: 0.2114 - val_mae: 0.2114\n",
            "Epoch 101/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1154 - mae: 0.1154 - val_loss: 0.1954 - val_mae: 0.1954\n",
            "Epoch 102/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1183 - mae: 0.1183 - val_loss: 0.1838 - val_mae: 0.1838\n",
            "Epoch 103/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1299 - mae: 0.1299 - val_loss: 0.2128 - val_mae: 0.2128\n",
            "Epoch 104/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1192 - mae: 0.1192 - val_loss: 0.1961 - val_mae: 0.1961\n",
            "Epoch 105/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1009 - mae: 0.1009 - val_loss: 0.1837 - val_mae: 0.1837\n",
            "Epoch 106/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1002 - mae: 0.1002 - val_loss: 0.1622 - val_mae: 0.1622\n",
            "Epoch 107/300\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1293 - mae: 0.1293 - val_loss: 0.1807 - val_mae: 0.1807\n",
            "Epoch 108/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1121 - mae: 0.1121 - val_loss: 0.2158 - val_mae: 0.2158\n",
            "Epoch 109/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1067 - mae: 0.1067 - val_loss: 0.2043 - val_mae: 0.2043\n",
            "Epoch 110/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1128 - mae: 0.1128 - val_loss: 0.2059 - val_mae: 0.2059\n",
            "Epoch 111/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1004 - mae: 0.1004 - val_loss: 0.2022 - val_mae: 0.2022\n",
            "Epoch 112/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1175 - mae: 0.1175 - val_loss: 0.1985 - val_mae: 0.1985\n",
            "Epoch 113/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1114 - mae: 0.1114 - val_loss: 0.1841 - val_mae: 0.1841\n",
            "Epoch 114/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1052 - mae: 0.1052 - val_loss: 0.1757 - val_mae: 0.1757\n",
            "Epoch 115/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0907 - mae: 0.0907 - val_loss: 0.1636 - val_mae: 0.1636\n",
            "Epoch 116/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1133 - mae: 0.1133 - val_loss: 0.2059 - val_mae: 0.2059\n",
            "Epoch 117/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1132 - mae: 0.1132 - val_loss: 0.1659 - val_mae: 0.1659\n",
            "Epoch 118/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1512 - mae: 0.1512 - val_loss: 0.1732 - val_mae: 0.1732\n",
            "Epoch 119/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0956 - mae: 0.0956 - val_loss: 0.1940 - val_mae: 0.1940\n",
            "Epoch 120/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1064 - mae: 0.1064 - val_loss: 0.1731 - val_mae: 0.1731\n",
            "Epoch 121/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0954 - mae: 0.0954 - val_loss: 0.1788 - val_mae: 0.1788\n",
            "Epoch 122/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0986 - mae: 0.0986 - val_loss: 0.1780 - val_mae: 0.1780\n",
            "Epoch 123/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1154 - mae: 0.1154 - val_loss: 0.1822 - val_mae: 0.1822\n",
            "Epoch 124/300\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1213 - mae: 0.1213 - val_loss: 0.1982 - val_mae: 0.1982\n",
            "Epoch 125/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1144 - mae: 0.1144 - val_loss: 0.1967 - val_mae: 0.1967\n",
            "Epoch 126/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1091 - mae: 0.1091 - val_loss: 0.1873 - val_mae: 0.1873\n",
            "Epoch 127/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1212 - mae: 0.1212 - val_loss: 0.1834 - val_mae: 0.1834\n",
            "Epoch 128/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1154 - mae: 0.1154 - val_loss: 0.1908 - val_mae: 0.1908\n",
            "Epoch 129/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0823 - mae: 0.0823 - val_loss: 0.1726 - val_mae: 0.1726\n",
            "Epoch 130/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1025 - mae: 0.1025 - val_loss: 0.1667 - val_mae: 0.1667\n",
            "Epoch 131/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1144 - mae: 0.1144 - val_loss: 0.1619 - val_mae: 0.1619\n",
            "Epoch 132/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1107 - mae: 0.1107 - val_loss: 0.1645 - val_mae: 0.1645\n",
            "Epoch 133/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0937 - mae: 0.0937 - val_loss: 0.1769 - val_mae: 0.1769\n",
            "Epoch 134/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0862 - mae: 0.0862 - val_loss: 0.1855 - val_mae: 0.1855\n",
            "Epoch 135/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1072 - mae: 0.1072 - val_loss: 0.1589 - val_mae: 0.1589\n",
            "Epoch 136/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1041 - mae: 0.1041 - val_loss: 0.1821 - val_mae: 0.1821\n",
            "Epoch 137/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1013 - mae: 0.1013 - val_loss: 0.1941 - val_mae: 0.1941\n",
            "Epoch 138/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0965 - mae: 0.0965 - val_loss: 0.1868 - val_mae: 0.1868\n",
            "Epoch 139/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1129 - mae: 0.1129 - val_loss: 0.1942 - val_mae: 0.1942\n",
            "Epoch 140/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0864 - mae: 0.0864 - val_loss: 0.2000 - val_mae: 0.2000\n",
            "Epoch 141/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1067 - mae: 0.1067 - val_loss: 0.1887 - val_mae: 0.1887\n",
            "Epoch 142/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1147 - mae: 0.1147 - val_loss: 0.1804 - val_mae: 0.1804\n",
            "Epoch 143/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1063 - mae: 0.1063 - val_loss: 0.1541 - val_mae: 0.1541\n",
            "Epoch 144/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0983 - mae: 0.0983 - val_loss: 0.1568 - val_mae: 0.1568\n",
            "Epoch 145/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0845 - mae: 0.0845 - val_loss: 0.1689 - val_mae: 0.1689\n",
            "Epoch 146/300\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.1037 - mae: 0.1037 - val_loss: 0.1852 - val_mae: 0.1852\n",
            "Epoch 147/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0959 - mae: 0.0959 - val_loss: 0.1719 - val_mae: 0.1719\n",
            "Epoch 148/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0964 - mae: 0.0964 - val_loss: 0.1553 - val_mae: 0.1553\n",
            "Epoch 149/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1089 - mae: 0.1089 - val_loss: 0.1575 - val_mae: 0.1575\n",
            "Epoch 150/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0853 - mae: 0.0853 - val_loss: 0.1596 - val_mae: 0.1596\n",
            "Epoch 151/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1127 - mae: 0.1127 - val_loss: 0.1661 - val_mae: 0.1661\n",
            "Epoch 152/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0996 - mae: 0.0996 - val_loss: 0.1702 - val_mae: 0.1702\n",
            "Epoch 153/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0999 - mae: 0.0999 - val_loss: 0.1597 - val_mae: 0.1597\n",
            "Epoch 154/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0954 - mae: 0.0954 - val_loss: 0.1446 - val_mae: 0.1446\n",
            "Epoch 155/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0914 - mae: 0.0914 - val_loss: 0.1758 - val_mae: 0.1758\n",
            "Epoch 156/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1089 - mae: 0.1089 - val_loss: 0.1895 - val_mae: 0.1895\n",
            "Epoch 157/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0941 - mae: 0.0941 - val_loss: 0.1485 - val_mae: 0.1485\n",
            "Epoch 158/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0954 - mae: 0.0954 - val_loss: 0.1623 - val_mae: 0.1623\n",
            "Epoch 159/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0865 - mae: 0.0865 - val_loss: 0.1709 - val_mae: 0.1709\n",
            "Epoch 160/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0937 - mae: 0.0937 - val_loss: 0.1646 - val_mae: 0.1646\n",
            "Epoch 161/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0963 - mae: 0.0963 - val_loss: 0.1637 - val_mae: 0.1637\n",
            "Epoch 162/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0945 - mae: 0.0945 - val_loss: 0.1739 - val_mae: 0.1739\n",
            "Epoch 163/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0998 - mae: 0.0998 - val_loss: 0.1752 - val_mae: 0.1752\n",
            "Epoch 164/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0944 - mae: 0.0944 - val_loss: 0.1765 - val_mae: 0.1765\n",
            "Epoch 165/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1258 - mae: 0.1258 - val_loss: 0.1520 - val_mae: 0.1520\n",
            "Epoch 166/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0911 - mae: 0.0911 - val_loss: 0.1736 - val_mae: 0.1736\n",
            "Epoch 167/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1171 - mae: 0.1171 - val_loss: 0.1923 - val_mae: 0.1923\n",
            "Epoch 168/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0871 - mae: 0.0871 - val_loss: 0.1781 - val_mae: 0.1781\n",
            "Epoch 169/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1168 - mae: 0.1168 - val_loss: 0.1665 - val_mae: 0.1665\n",
            "Epoch 170/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1143 - mae: 0.1143 - val_loss: 0.1721 - val_mae: 0.1721\n",
            "Epoch 171/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0878 - mae: 0.0878 - val_loss: 0.1607 - val_mae: 0.1607\n",
            "Epoch 172/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0932 - mae: 0.0932 - val_loss: 0.1736 - val_mae: 0.1736\n",
            "Epoch 173/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0901 - mae: 0.0901 - val_loss: 0.1656 - val_mae: 0.1656\n",
            "Epoch 174/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1014 - mae: 0.1014 - val_loss: 0.1580 - val_mae: 0.1580\n",
            "Epoch 175/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0845 - mae: 0.0845 - val_loss: 0.1603 - val_mae: 0.1603\n",
            "Epoch 176/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1482 - mae: 0.1482 - val_loss: 0.1584 - val_mae: 0.1584\n",
            "Epoch 177/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1127 - mae: 0.1127 - val_loss: 0.1792 - val_mae: 0.1792\n",
            "Epoch 178/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0861 - mae: 0.0861 - val_loss: 0.1731 - val_mae: 0.1731\n",
            "Epoch 179/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1050 - mae: 0.1050 - val_loss: 0.1547 - val_mae: 0.1547\n",
            "Epoch 180/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0922 - mae: 0.0922 - val_loss: 0.1714 - val_mae: 0.1714\n",
            "Epoch 181/300\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1037 - mae: 0.1037 - val_loss: 0.1981 - val_mae: 0.1981\n",
            "Epoch 182/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1027 - mae: 0.1027 - val_loss: 0.1584 - val_mae: 0.1584\n",
            "Epoch 183/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1182 - mae: 0.1182 - val_loss: 0.1544 - val_mae: 0.1544\n",
            "Epoch 184/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1177 - mae: 0.1177 - val_loss: 0.1699 - val_mae: 0.1699\n",
            "Epoch 185/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0899 - mae: 0.0899 - val_loss: 0.1608 - val_mae: 0.1608\n",
            "Epoch 186/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0935 - mae: 0.0935 - val_loss: 0.1628 - val_mae: 0.1628\n",
            "Epoch 187/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0931 - mae: 0.0931 - val_loss: 0.1590 - val_mae: 0.1590\n",
            "Epoch 188/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0896 - mae: 0.0896 - val_loss: 0.1673 - val_mae: 0.1673\n",
            "Epoch 189/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0940 - mae: 0.0940 - val_loss: 0.1574 - val_mae: 0.1574\n",
            "Epoch 190/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0996 - mae: 0.0996 - val_loss: 0.1477 - val_mae: 0.1477\n",
            "Epoch 191/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0872 - mae: 0.0872 - val_loss: 0.1659 - val_mae: 0.1659\n",
            "Epoch 192/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0990 - mae: 0.0990 - val_loss: 0.1671 - val_mae: 0.1671\n",
            "Epoch 193/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0966 - mae: 0.0966 - val_loss: 0.1604 - val_mae: 0.1604\n",
            "Epoch 194/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0848 - mae: 0.0848 - val_loss: 0.1557 - val_mae: 0.1557\n",
            "Epoch 195/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0965 - mae: 0.0965 - val_loss: 0.1456 - val_mae: 0.1456\n",
            "Epoch 196/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0880 - mae: 0.0880 - val_loss: 0.1629 - val_mae: 0.1629\n",
            "Epoch 197/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0801 - mae: 0.0801 - val_loss: 0.1769 - val_mae: 0.1769\n",
            "Epoch 198/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0696 - mae: 0.0696 - val_loss: 0.1740 - val_mae: 0.1740\n",
            "Epoch 199/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0991 - mae: 0.0991 - val_loss: 0.1611 - val_mae: 0.1611\n",
            "Epoch 200/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0855 - mae: 0.0855 - val_loss: 0.1759 - val_mae: 0.1759\n",
            "Epoch 201/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0976 - mae: 0.0976 - val_loss: 0.1755 - val_mae: 0.1755\n",
            "Epoch 202/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1004 - mae: 0.1004 - val_loss: 0.1561 - val_mae: 0.1561\n",
            "Epoch 203/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1228 - mae: 0.1228 - val_loss: 0.1742 - val_mae: 0.1742\n",
            "Epoch 204/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1110 - mae: 0.1110 - val_loss: 0.1799 - val_mae: 0.1799\n",
            "Epoch 205/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0972 - mae: 0.0972 - val_loss: 0.1573 - val_mae: 0.1573\n",
            "Epoch 206/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0952 - mae: 0.0952 - val_loss: 0.1767 - val_mae: 0.1767\n",
            "Epoch 207/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0842 - mae: 0.0842 - val_loss: 0.1653 - val_mae: 0.1653\n",
            "Epoch 208/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0975 - mae: 0.0975 - val_loss: 0.1634 - val_mae: 0.1634\n",
            "Epoch 209/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0889 - mae: 0.0889 - val_loss: 0.1618 - val_mae: 0.1618\n",
            "Epoch 210/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1106 - mae: 0.1106 - val_loss: 0.1555 - val_mae: 0.1555\n",
            "Epoch 211/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0731 - mae: 0.0731 - val_loss: 0.1608 - val_mae: 0.1608\n",
            "Epoch 212/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1073 - mae: 0.1073 - val_loss: 0.1803 - val_mae: 0.1803\n",
            "Epoch 213/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1143 - mae: 0.1143 - val_loss: 0.1625 - val_mae: 0.1625\n",
            "Epoch 214/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0836 - mae: 0.0836 - val_loss: 0.1702 - val_mae: 0.1702\n",
            "Epoch 215/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0884 - mae: 0.0884 - val_loss: 0.1601 - val_mae: 0.1601\n",
            "Epoch 216/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0952 - mae: 0.0952 - val_loss: 0.1723 - val_mae: 0.1723\n",
            "Epoch 217/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0903 - mae: 0.0903 - val_loss: 0.1517 - val_mae: 0.1517\n",
            "Epoch 218/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0964 - mae: 0.0964 - val_loss: 0.1755 - val_mae: 0.1755\n",
            "Epoch 219/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0876 - mae: 0.0876 - val_loss: 0.1627 - val_mae: 0.1627\n",
            "Epoch 220/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0922 - mae: 0.0922 - val_loss: 0.1482 - val_mae: 0.1482\n",
            "Epoch 221/300\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.0735 - mae: 0.0735 - val_loss: 0.1414 - val_mae: 0.1414\n",
            "Epoch 222/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0962 - mae: 0.0962 - val_loss: 0.1765 - val_mae: 0.1765\n",
            "Epoch 223/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0919 - mae: 0.0919 - val_loss: 0.1610 - val_mae: 0.1610\n",
            "Epoch 224/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1043 - mae: 0.1043 - val_loss: 0.1553 - val_mae: 0.1553\n",
            "Epoch 225/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0762 - mae: 0.0762 - val_loss: 0.1607 - val_mae: 0.1607\n",
            "Epoch 226/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0819 - mae: 0.0819 - val_loss: 0.1687 - val_mae: 0.1687\n",
            "Epoch 227/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0910 - mae: 0.0910 - val_loss: 0.1626 - val_mae: 0.1626\n",
            "Epoch 228/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0942 - mae: 0.0942 - val_loss: 0.1475 - val_mae: 0.1475\n",
            "Epoch 229/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0935 - mae: 0.0935 - val_loss: 0.1609 - val_mae: 0.1609\n",
            "Epoch 230/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0888 - mae: 0.0888 - val_loss: 0.1557 - val_mae: 0.1557\n",
            "Epoch 231/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0793 - mae: 0.0793 - val_loss: 0.1685 - val_mae: 0.1685\n",
            "Epoch 232/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0718 - mae: 0.0718 - val_loss: 0.1397 - val_mae: 0.1397\n",
            "Epoch 233/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0870 - mae: 0.0870 - val_loss: 0.1519 - val_mae: 0.1519\n",
            "Epoch 234/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0863 - mae: 0.0863 - val_loss: 0.1552 - val_mae: 0.1552\n",
            "Epoch 235/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0981 - mae: 0.0981 - val_loss: 0.1306 - val_mae: 0.1306\n",
            "Epoch 236/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0928 - mae: 0.0928 - val_loss: 0.1574 - val_mae: 0.1574\n",
            "Epoch 237/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0931 - mae: 0.0931 - val_loss: 0.1682 - val_mae: 0.1682\n",
            "Epoch 238/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0917 - mae: 0.0917 - val_loss: 0.1628 - val_mae: 0.1628\n",
            "Epoch 239/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0985 - mae: 0.0985 - val_loss: 0.1534 - val_mae: 0.1534\n",
            "Epoch 240/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0919 - mae: 0.0919 - val_loss: 0.1443 - val_mae: 0.1443\n",
            "Epoch 241/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1001 - mae: 0.1001 - val_loss: 0.1452 - val_mae: 0.1452\n",
            "Epoch 242/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0819 - mae: 0.0819 - val_loss: 0.1454 - val_mae: 0.1454\n",
            "Epoch 243/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0943 - mae: 0.0943 - val_loss: 0.1622 - val_mae: 0.1622\n",
            "Epoch 244/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0643 - mae: 0.0643 - val_loss: 0.1668 - val_mae: 0.1668\n",
            "Epoch 245/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0968 - mae: 0.0968 - val_loss: 0.1576 - val_mae: 0.1576\n",
            "Epoch 246/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0723 - mae: 0.0723 - val_loss: 0.1561 - val_mae: 0.1561\n",
            "Epoch 247/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1025 - mae: 0.1025 - val_loss: 0.1485 - val_mae: 0.1485\n",
            "Epoch 248/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0896 - mae: 0.0896 - val_loss: 0.1639 - val_mae: 0.1639\n",
            "Epoch 249/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1089 - mae: 0.1089 - val_loss: 0.1686 - val_mae: 0.1686\n",
            "Epoch 250/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0851 - mae: 0.0851 - val_loss: 0.1475 - val_mae: 0.1475\n",
            "Epoch 251/300\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0919 - mae: 0.0919 - val_loss: 0.1416 - val_mae: 0.1416\n",
            "Epoch 252/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1077 - mae: 0.1077 - val_loss: 0.1755 - val_mae: 0.1755\n",
            "Epoch 253/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0882 - mae: 0.0882 - val_loss: 0.1466 - val_mae: 0.1466\n",
            "Epoch 254/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0858 - mae: 0.0858 - val_loss: 0.1439 - val_mae: 0.1439\n",
            "Epoch 255/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0919 - mae: 0.0919 - val_loss: 0.1385 - val_mae: 0.1385\n",
            "Epoch 256/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0757 - mae: 0.0757 - val_loss: 0.1407 - val_mae: 0.1407\n",
            "Epoch 257/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0857 - mae: 0.0857 - val_loss: 0.1667 - val_mae: 0.1667\n",
            "Epoch 258/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0929 - mae: 0.0929 - val_loss: 0.1738 - val_mae: 0.1738\n",
            "Epoch 259/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0917 - mae: 0.0917 - val_loss: 0.1587 - val_mae: 0.1587\n",
            "Epoch 260/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0848 - mae: 0.0848 - val_loss: 0.1565 - val_mae: 0.1565\n",
            "Epoch 261/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0892 - mae: 0.0892 - val_loss: 0.1767 - val_mae: 0.1767\n",
            "Epoch 262/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1226 - mae: 0.1226 - val_loss: 0.1333 - val_mae: 0.1333\n",
            "Epoch 263/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1000 - mae: 0.1000 - val_loss: 0.1431 - val_mae: 0.1431\n",
            "Epoch 264/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0866 - mae: 0.0866 - val_loss: 0.1629 - val_mae: 0.1629\n",
            "Epoch 265/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1174 - mae: 0.1174 - val_loss: 0.1613 - val_mae: 0.1613\n",
            "Epoch 266/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0930 - mae: 0.0930 - val_loss: 0.1711 - val_mae: 0.1711\n",
            "Epoch 267/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1016 - mae: 0.1016 - val_loss: 0.1598 - val_mae: 0.1598\n",
            "Epoch 268/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1031 - mae: 0.1031 - val_loss: 0.1646 - val_mae: 0.1646\n",
            "Epoch 269/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0736 - mae: 0.0736 - val_loss: 0.1546 - val_mae: 0.1546\n",
            "Epoch 270/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.0927 - val_loss: 0.1659 - val_mae: 0.1659\n",
            "Epoch 271/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0830 - mae: 0.0830 - val_loss: 0.1543 - val_mae: 0.1543\n",
            "Epoch 272/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0777 - mae: 0.0777 - val_loss: 0.1723 - val_mae: 0.1723\n",
            "Epoch 273/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0821 - mae: 0.0821 - val_loss: 0.1784 - val_mae: 0.1784\n",
            "Epoch 274/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0794 - mae: 0.0794 - val_loss: 0.1589 - val_mae: 0.1589\n",
            "Epoch 275/300\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0809 - mae: 0.0809 - val_loss: 0.1329 - val_mae: 0.1329\n",
            "Epoch 276/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0838 - mae: 0.0838 - val_loss: 0.1676 - val_mae: 0.1676\n",
            "Epoch 277/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0851 - mae: 0.0851 - val_loss: 0.1493 - val_mae: 0.1493\n",
            "Epoch 278/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0795 - mae: 0.0795 - val_loss: 0.1886 - val_mae: 0.1886\n",
            "Epoch 279/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1018 - mae: 0.1018 - val_loss: 0.1597 - val_mae: 0.1597\n",
            "Epoch 280/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0836 - mae: 0.0836 - val_loss: 0.1205 - val_mae: 0.1205\n",
            "Epoch 281/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0800 - mae: 0.0800 - val_loss: 0.1591 - val_mae: 0.1591\n",
            "Epoch 282/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0736 - mae: 0.0736 - val_loss: 0.1444 - val_mae: 0.1444\n",
            "Epoch 283/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0825 - mae: 0.0825 - val_loss: 0.1411 - val_mae: 0.1411\n",
            "Epoch 284/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0872 - mae: 0.0872 - val_loss: 0.1387 - val_mae: 0.1387\n",
            "Epoch 285/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0812 - mae: 0.0812 - val_loss: 0.1465 - val_mae: 0.1465\n",
            "Epoch 286/300\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0794 - mae: 0.0794 - val_loss: 0.1530 - val_mae: 0.1530\n",
            "Epoch 287/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0872 - mae: 0.0872 - val_loss: 0.1447 - val_mae: 0.1447\n",
            "Epoch 288/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1095 - mae: 0.1095 - val_loss: 0.1366 - val_mae: 0.1366\n",
            "Epoch 289/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0785 - mae: 0.0785 - val_loss: 0.1308 - val_mae: 0.1308\n",
            "Epoch 290/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0791 - mae: 0.0791 - val_loss: 0.1413 - val_mae: 0.1413\n",
            "Epoch 291/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0867 - mae: 0.0867 - val_loss: 0.1310 - val_mae: 0.1310\n",
            "Epoch 292/300\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0748 - mae: 0.0748 - val_loss: 0.1367 - val_mae: 0.1367\n",
            "Epoch 293/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0890 - mae: 0.0890 - val_loss: 0.1357 - val_mae: 0.1357\n",
            "Epoch 294/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0626 - mae: 0.0626 - val_loss: 0.1281 - val_mae: 0.1281\n",
            "Epoch 295/300\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.0895 - mae: 0.0895 - val_loss: 0.1322 - val_mae: 0.1322\n",
            "Epoch 296/300\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0853 - mae: 0.0853 - val_loss: 0.1429 - val_mae: 0.1429\n",
            "Epoch 297/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0741 - mae: 0.0741 - val_loss: 0.1605 - val_mae: 0.1605\n",
            "Epoch 298/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0818 - mae: 0.0818 - val_loss: 0.1657 - val_mae: 0.1657\n",
            "Epoch 299/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0939 - mae: 0.0939 - val_loss: 0.1447 - val_mae: 0.1447\n",
            "Epoch 300/300\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1005 - mae: 0.1005 - val_loss: 0.1546 - val_mae: 0.1546\n",
            "Model: \"sequential_121\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_121 (Flatten)        (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "dense_605 (Dense)            (None, 46)                276       \n",
            "_________________________________________________________________\n",
            "dense_606 (Dense)            (None, 34)                1598      \n",
            "_________________________________________________________________\n",
            "dense_607 (Dense)            (None, 28)                980       \n",
            "_________________________________________________________________\n",
            "dense_608 (Dense)            (None, 33)                957       \n",
            "_________________________________________________________________\n",
            "dropout_121 (Dropout)        (None, 33)                0         \n",
            "_________________________________________________________________\n",
            "dense_609 (Dense)            (None, 1)                 34        \n",
            "=================================================================\n",
            "Total params: 3,845\n",
            "Trainable params: 3,845\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.87386036]\n",
            " [0.50358033]\n",
            " [0.39900303]\n",
            " [0.4125924 ]\n",
            " [0.00455457]\n",
            " [0.30115223]\n",
            " [0.06647325]\n",
            " [0.12562633]\n",
            " [0.04529142]], shape=(9, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.05333946]\n",
            " [0.1361028 ]\n",
            " [0.08254097]\n",
            " [0.09038169]\n",
            " [0.00709435]\n",
            " [0.11120836]\n",
            " [0.03992387]\n",
            " [0.04193135]\n",
            " [0.01572619]], shape=(9, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTqkyjoflq7T",
        "outputId": "e19b59b5-eeac-4f19-a414-721ae51e9795"
      },
      "source": [
        "!pip3 install pygad"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygad\n",
            "  Downloading pygad-2.16.0-py3-none-any.whl (52 kB)\n",
            "\u001b[?25l\r\u001b[K     |                         | 10 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |                   | 20 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |             | 30 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |       | 40 kB 30.8 MB/s eta 0:00:01\r\u001b[K     | | 51 kB 15.9 MB/s eta 0:00:01\r\u001b[K     || 52 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pygad) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pygad) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pygad) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pygad) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pygad) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pygad) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->pygad) (1.15.0)\n",
            "Installing collected packages: pygad\n",
            "Successfully installed pygad-2.16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TFENw60Zlq91",
        "outputId": "1c114a30-63cc-4269-8963-8c0fe8100e80"
      },
      "source": [
        "# Optimization for input features\n",
        "\n",
        "import numpy as np\n",
        "import pygad\n",
        "\n",
        "#initialization\n",
        "function_inputs = [0.5,0.5,0.5,0.5,0.5]\n",
        "\n",
        "#define fitness function\n",
        "def fitness_func(solution, solution_idx):\n",
        "    target = [1.5]\n",
        "    target = abs(y-y_mean)/y_std\n",
        "    output=model(np.array(solution*function_inputs).reshape(1,5)) # model is built in keras.tuner\n",
        "    output = abs(tf.subtract(abs(output), abs(target)))\n",
        "    fitness =((1/output)).numpy()[0]\n",
        "    #fitness =output.numpy()[0]\n",
        "    return fitness\n",
        "\n",
        "# Number of generations.\n",
        "num_generations = 100\n",
        "# Number of solutions to be selected as parents in the mating pool.\n",
        "num_parents_mating = 5 \n",
        "# Number of solutions in the population.\n",
        "sol_per_pop = 10 \n",
        "  \n",
        "# Type of parent selection.\n",
        "parent_selection_type = \"sss\"\n",
        "# Type of the crossover operator.\n",
        "crossover_type = \"single_point\" \n",
        "# Type of the mutation operator.\n",
        "mutation_type = \"random\"\n",
        "\n",
        "last_fitness = 0\n",
        "def callback_generation(ga_instance):\n",
        "    global last_fitness\n",
        "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
        "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
        "    print(\"Change     = {change}\".format(change=ga_instance.best_solution()[1] - last_fitness))\n",
        "    last_fitness = ga_instance.best_solution()[1]\n",
        "\n",
        "# Creating an instance of the GA class inside the ga module. Some parameters are initialized within the constructor.\n",
        "ga_instance = pygad.GA(\n",
        "     #Number of generations(iterations).\n",
        "     num_generations=num_generations,\n",
        "     num_parents_mating=num_parents_mating,\n",
        "     fitness_func=fitness_func,\n",
        "     sol_per_pop=sol_per_pop,\n",
        "     #the Number of parameters(the number of genes)\n",
        "     num_genes=5,\n",
        "     gene_space=[np.arange(-1.68,2.88,0.01).tolist(),np.arange(-1.34,1.73,0.01).tolist(),np.arange(-0.88,1.56,0.01).tolist(),np.arange(-0.99,1.32,0.01).tolist(),\n",
        "                 np.arange(-1.26,-0.392,0.01).tolist()],\n",
        "     \n",
        "     parent_selection_type=parent_selection_type,\n",
        "     keep_parents=1,\n",
        "     crossover_type=crossover_type,\n",
        "     mutation_percent_genes=50,\n",
        "     mutation_type=mutation_type,\n",
        "     callback_generation=callback_generation)\n",
        "\n",
        "# Running the GA to optimize the parameters of the function.\n",
        "ga_instance.run()\n",
        "\n",
        "\n",
        "# After the generations complete, some plots are showed that summarize the how the outputs/fitenss values evolve over generations.\n",
        "ga_instance.plot_result()\n",
        "\n",
        "# Returning the details of the best solution.\n",
        "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "solution = tf.multiply(solution,x_std)\n",
        "solution = tf.add(solution, x_mean)\n",
        "print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
        "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=1/solution_fitness))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pygad/pygad.py:731: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.\n",
            "  if not self.suppress_warnings: warnings.warn(\"Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generation = 1\n",
            "Fitness    = [3.908528]\n",
            "Change     = [3.908528]\n",
            "Generation = 2\n",
            "Fitness    = [3.908528]\n",
            "Change     = [0.]\n",
            "Generation = 3\n",
            "Fitness    = [3.9344234]\n",
            "Change     = [0.02589536]\n",
            "Generation = 4\n",
            "Fitness    = [3.9344234]\n",
            "Change     = [0.]\n",
            "Generation = 5\n",
            "Fitness    = [3.982496]\n",
            "Change     = [0.04807258]\n",
            "Generation = 6\n",
            "Fitness    = [3.982496]\n",
            "Change     = [0.]\n",
            "Generation = 7\n",
            "Fitness    = [3.982496]\n",
            "Change     = [0.]\n",
            "Generation = 8\n",
            "Fitness    = [3.9932117]\n",
            "Change     = [0.01071572]\n",
            "Generation = 9\n",
            "Fitness    = [3.9932117]\n",
            "Change     = [0.]\n",
            "Generation = 10\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.02260828]\n",
            "Generation = 11\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 12\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 13\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 14\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 15\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 16\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 17\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 18\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 19\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 20\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 21\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 22\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 23\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 24\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 25\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 26\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 27\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 28\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 29\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 30\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 31\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 32\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 33\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 34\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 35\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 36\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 37\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 38\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 39\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 40\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 41\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 42\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 43\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 44\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 45\n",
            "Fitness    = [4.01582]\n",
            "Change     = [0.]\n",
            "Generation = 46\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.20346403]\n",
            "Generation = 47\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 48\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 49\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 50\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 51\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 52\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 53\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 54\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 55\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 56\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 57\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 58\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 59\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 60\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 61\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 62\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 63\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 64\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 65\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 66\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 67\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 68\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 69\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 70\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 71\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 72\n",
            "Fitness    = [4.219284]\n",
            "Change     = [0.]\n",
            "Generation = 73\n",
            "Fitness    = [4.294001]\n",
            "Change     = [0.07471704]\n",
            "Generation = 74\n",
            "Fitness    = [4.294001]\n",
            "Change     = [0.]\n",
            "Generation = 75\n",
            "Fitness    = [4.294001]\n",
            "Change     = [0.]\n",
            "Generation = 76\n",
            "Fitness    = [4.294001]\n",
            "Change     = [0.]\n",
            "Generation = 77\n",
            "Fitness    = [4.294001]\n",
            "Change     = [0.]\n",
            "Generation = 78\n",
            "Fitness    = [4.294001]\n",
            "Change     = [0.]\n",
            "Generation = 79\n",
            "Fitness    = [4.294001]\n",
            "Change     = [0.]\n",
            "Generation = 80\n",
            "Fitness    = [4.294001]\n",
            "Change     = [0.]\n",
            "Generation = 81\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.5193987]\n",
            "Generation = 82\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 83\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 84\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 85\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 86\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 87\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 88\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 89\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 90\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 91\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 92\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 93\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 94\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 95\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 96\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 97\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 98\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 99\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n",
            "Generation = 100\n",
            "Fitness    = [4.8134]\n",
            "Change     = [0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pygad/pygad.py:3105: UserWarning: Please use the plot_fitness() method instead of plot_result(). The plot_result() method will be removed in the future.\n",
            "  warnings.warn(\"Please use the plot_fitness() method instead of plot_result(). The plot_result() method will be removed in the future.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Parameters of the best solution : [11.71565336 13.74696652 78.90247369 69.26924224  4.80344359]\n",
            "Fitness value of the best solution = [0.20775336]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lib4n0wSlrAc"
      },
      "source": [
        "save_path = r'E:\\model\\group13(13.8).h5'\n",
        "model.save(save_path)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTAL7VpClrDE",
        "outputId": "9845dbe7-ba94-48d6-eec9-acc3e85a63cf"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[15.509139  ]\n",
            " [ 4.2035804 ]\n",
            " [ 4.434997  ]\n",
            " [ 4.1524076 ]\n",
            " [ 0.63744545]\n",
            " [ 2.4068477 ]\n",
            " [ 1.7314732 ]\n",
            " [ 3.1216264 ]\n",
            " [ 2.8347087 ]], shape=(9, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzNwsxit9wfK"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "figsize = 8,5\n",
        "figure, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "x = np.array([1,2,3,4,5,6,7,8,9])\n",
        "y1 = np.array([16.383, 3.7, 4.834, 4.565, 0.642, 2.708, 1.665, 2.996, 2.88])\n",
        "y2 = np.array([14.461, 3.437, 3.995, 3.758, 0.727, 2.615, 1.575, 3.076, 2.815])\n",
        "s1 = plt.scatter(x,y1, s=50, c= 'b', marker='^')\n",
        "s2 = plt.scatter(x,y2, s=50, c= 'r', marker='^')\n",
        "plt.xlabel('Number of group',fontsize=18)\n",
        "plt.ylabel('Value', fontsize=18)\n",
        "plt.title('The results comparison',fontsize=18)\n",
        "plt.legend((s1,s2),('experimental results','prediction results') ,loc = 'best',fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}